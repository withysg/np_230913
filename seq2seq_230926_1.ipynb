{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3320) (1001, 664)\n",
      "3320 8 664\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset _Mix\n",
    "# X = np.load('EBR32094107_BOT_Head100_x.npy', allow_pickle=True)  \n",
    "## X = X[:, :15] 으로 학습 시, Epoch [31/100000], Train Loss: 0.00883, Train Acc: 0.911, Valid Loss: 0.02390, Valid Acc: 0.902\n",
    "# y = np.load('All_Head_Simple_NoMix_y.npy')\n",
    "# category_var = np.load('EBR32094107_BOT_Head100_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = np.load('All_Head_Simple_NoMix_x.npy')  # 여기 데이터셋으로 검증하면 정확도 0.89 임\n",
    "# category_var = np.load('All_Head_Simple_NoMix_y.npy')\n",
    "\n",
    "# X = np.load('All_Head_Simple_x.npy')\n",
    "# category_var = np.load('All_Head_Simple_y.npy')\n",
    "\n",
    "# X = np.load('D:/py/DGS/EBR32094107_BOT_Head66_x.npy', allow_pickle=True)\n",
    "# category_var = np.load('D:/py/DGS/EBR32094107_BOT_Head66_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = np.load('D:/py/DGS/EBR32094107_BOT_Head_NoMix_x.npy', allow_pickle=True)\n",
    "# category_var = np.load('D:/py/DGS/EBR32094107_BOT_Head_NoMix_y.npy', allow_pickle=True)\n",
    "\n",
    "######################################################\n",
    "#### 학습 / 검증용 #########\n",
    "# X = np.load('Head_230921_2_x.npy', allow_pickle=True)\n",
    "# # X = X[ : , :14]  # 테스트용 동일 모델에서는 옵션값만 있으면 학습이되는지 확인하기 위함 : 결과) 동일 모델로 구성된 데이터셋에서 부품정보없이, 옵션정보만 입력해도 예측율 동일 수준 임\n",
    "\n",
    "# # category_var = np.load('Head_230921_2_y.npy', allow_pickle=True)\n",
    "# # print(category_var[:10])\n",
    "\n",
    "\n",
    "# X = X.astype(np.int32)\n",
    "# # category_var = category_var.astype(np.int32)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################\n",
    "#### 학습 / 검증용  Shuffle ############\n",
    "X = np.load('easydata_230921_2_x.npy', allow_pickle=True)\n",
    "# X = X[ : , :14]  # 테스트용 동일 모델에서는 옵션값만 있으면 학습이되는지 확인하기 위함 : 결과) 동일 모델로 구성된 데이터셋에서 부품정보없이, 옵션정보만 입력해도 예측율 동일 수준 임\n",
    "\n",
    "# category_var = np.load('Head_230921_2_y.npy', allow_pickle=True)\n",
    "# # print(category_var[:10])\n",
    "\n",
    "\n",
    "X = X.astype(np.int32)\n",
    "# category_var = category_var.astype(np.int)\n",
    "\n",
    "# num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "y = np.load('easydata_230921_2_y.npy', allow_pickle=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = y.reshape(-1)\n",
    "y = le.fit_transform(y)\n",
    "y= y. reshape(1001, 664)\n",
    "\n",
    "y = y.astype('int')\n",
    "print(X.shape , y.shape)\n",
    "X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_test)\n",
    "X_train_scaled = X_train\n",
    "X_valid_scaled = X_test\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######## Test용 ############\n",
    "# X = np.load('Head_230921_2_test_x.npy', allow_pickle=True)\n",
    "# # category_var = np.load('Head_230921_2_test_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = X.astype(np.int32)\n",
    "# # category_var = category_var.astype(np.int)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_test_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.99, random_state=42)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# # scaler = MinMaxScaler()\n",
    "# scaler = load(open('MinMax_scaler_Head_230921.pkl', 'rb'))    #Scaler 불러오기\n",
    "\n",
    "# ## X_train_scaled = scaler.fit_transform(X_train)    # Test 시, 사용하면 안됨\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "# print(X_valid_scaled.shape)\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######## NoTrain 용 ############\n",
    "# X = np.load('Head_230921_2_NoTrain_x.npy', allow_pickle=True)\n",
    "# # category_var = np.load('Head_230921_2_NoTrain_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = X.astype(np.int32)\n",
    "# # category_var = category_var.astype(np.int)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_NoTrain_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_test = X\n",
    "# y_valid = y\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# # scaler = MinMaxScaler()\n",
    "# scaler = load(open('MinMax_scaler_Head_230921.pkl', 'rb'))    #Scaler 불러오기\n",
    "\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "# # print(X_valid_scaled.shape)\n",
    "###########################################################################\n",
    "\n",
    "inputsize = X.shape[1]\n",
    "num_classes = len(set(y[0]))\n",
    "outsize = len(y[0])\n",
    "\n",
    "print(inputsize , num_classes , outsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class AssemblyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = AssemblyDataset(X_train_scaled, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = AssemblyDataset(X_valid_scaled, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq 모델 정의\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_outsize , num_classes):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        self.decoder = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_outsize * num_classes)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.embedding(src.long())  # 입력 텐서를 Long 타입으로 변환\n",
    "        embedded_tgt = self.embedding(tgt.long())  # 입력 텐서를 Long 타입으로 변환\n",
    "\n",
    "        encoder_out, hidden = self.encoder(embedded_src)\n",
    "        decoder_out, _ = self.decoder(embedded_tgt, hidden)\n",
    "\n",
    "        out = self.fc(decoder_out[:, -1, :])\n",
    "        out = out.view(-1, num_outsize , num_classes)\n",
    "        # print(out.shape)\n",
    "\n",
    "        # out = self.fc(decoder_out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = inputsize  #100  # 입력 데이터의 크기\n",
    "hidden_size = 256  # 256 임베딩 및 트랜스포머 hidden state의 크기 , 임베딩 사이즈는 num_heads로 나누어 질 수 있어야 함\n",
    "num_layers = 4  # 4 트랜스포머 레이어의 수\n",
    "num_classes = num_classes #  # 클래스의 수\n",
    "num_outsize = outsize   \n",
    "num_heads = 8  # 8 멀티 헤드 어텐션의 헤드 수\n",
    "dropout = 1  # 드롭아웃 비율 , 소수점 입력 시 에러 발생 TypeError: 'float' object cannot be interpreted as an integer\n",
    "batch_size = 128  # 배치 크기\n",
    "# num_epochs = 10  # 에포크 수\n",
    "\n",
    "model = Seq2SeqModel(input_size, hidden_size, num_layers, num_outsize , num_classes).to(device)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "num_epochs = 100000\n",
    "acc_compare = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct_t = 0\n",
    "    total_t = 0\n",
    "\n",
    "    for batch_input, batch_output_t in train_loader:\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output_t = batch_output_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch_input.shape , batch_input.dtype)\n",
    "        # outputs = model(batch_input.unsqueeze(1))\n",
    "        # outputs = model(batch_input)\n",
    "        outputs = model(batch_input, batch_input)\n",
    "\n",
    "        # 손실 계산\n",
    "        # print(outputs.view(-1 , 10).shape, batch_output_t.view(-1).shape , batch_output_t.shape)\n",
    "        loss = criterion(outputs.view(-1 , num_classes), batch_output_t.view(-1) )  # 모양을 맞춤\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        predicted_t = outputs.argmax(dim=2)\n",
    "#         print(predicted , batch_output)\n",
    "        correct_t += (predicted_t == batch_output_t).sum().item()\n",
    "        total_t += batch_output_t.shape[0]*batch_output_t.shape[1]\n",
    "        # print((predicted_t == batch_output_t).sum().item() , batch_output_t.shape[0]*batch_output_t.shape[1])\n",
    "#         print(predicted_t , batch_output_t)\n",
    "\n",
    "        # print(outputs.view(-1 , num_classes).shape , predicted_t.shape , batch_output_t.shape)\n",
    "        # batch_output_t = batch_output_t.reshape(-1)\n",
    "        # predicted_t = predicted_t.reshape(-1)\n",
    "        # for i in range(len(batch_output_t)):\n",
    "        #         if batch_output_t[i] != 0:\n",
    "        #                 total_t+=1\n",
    "        #                 if batch_output_t[i] == predicted_t[i]:\n",
    "        #                         correct_t+=1\n",
    "\n",
    "#     average_loss = total_loss / len(train_loader)\n",
    "#     acc_train = correct_t / total_t\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f} , acc : { acc_train :.4f} , { correct_t , total_t}')\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_t / (total_t)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f} , acc : { accuracy :.4f} , { correct_t , total_t}')\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in valid_loader:\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_output = batch_output.to(device)\n",
    "            outputs = model(batch_input , batch_input)\n",
    "            predicted = outputs.argmax(dim=2)\n",
    "    #         print(predicted , batch_output)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "    #         print(batch_output.size(0))\n",
    "            total += batch_output.shape[0]*batch_output.shape[1]\n",
    "\n",
    "\n",
    "        #     batch_output = batch_output.reshape(-1)\n",
    "        #     predicted = predicted.reshape(-1)\n",
    "        #     for i in range(len(batch_output)):\n",
    "        #         if batch_output[i] != 0:\n",
    "        #                 total_v+=1\n",
    "        #                 if batch_output[i] == predicted[i]:\n",
    "        #                         correct_v+=1\n",
    "\n",
    "        # acc_val = correct_v / total_v\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}% , {correct , total}')\n",
    "    if accuracy > acc_compare:\n",
    "        acc_compare = accuracy\n",
    "        torch.save(model.state_dict(), \"seq2seq_230926_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
