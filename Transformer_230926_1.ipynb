{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 3320) (1001, 664)\n",
      "3320 8 664\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset _Mix\n",
    "# X = np.load('EBR32094107_BOT_Head100_x.npy', allow_pickle=True)  \n",
    "## X = X[:, :15] 으로 학습 시, Epoch [31/100000], Train Loss: 0.00883, Train Acc: 0.911, Valid Loss: 0.02390, Valid Acc: 0.902\n",
    "# y = np.load('All_Head_Simple_NoMix_y.npy')\n",
    "# category_var = np.load('EBR32094107_BOT_Head100_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = np.load('All_Head_Simple_NoMix_x.npy')  # 여기 데이터셋으로 검증하면 정확도 0.89 임\n",
    "# category_var = np.load('All_Head_Simple_NoMix_y.npy')\n",
    "\n",
    "# X = np.load('All_Head_Simple_x.npy')\n",
    "# category_var = np.load('All_Head_Simple_y.npy')\n",
    "\n",
    "# X = np.load('D:/py/DGS/EBR32094107_BOT_Head66_x.npy', allow_pickle=True)\n",
    "# category_var = np.load('D:/py/DGS/EBR32094107_BOT_Head66_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = np.load('D:/py/DGS/EBR32094107_BOT_Head_NoMix_x.npy', allow_pickle=True)\n",
    "# category_var = np.load('D:/py/DGS/EBR32094107_BOT_Head_NoMix_y.npy', allow_pickle=True)\n",
    "\n",
    "######################################################\n",
    "#### 학습 / 검증용 #########\n",
    "# X = np.load('Head_230921_2_x.npy', allow_pickle=True)\n",
    "# # X = X[ : , :14]  # 테스트용 동일 모델에서는 옵션값만 있으면 학습이되는지 확인하기 위함 : 결과) 동일 모델로 구성된 데이터셋에서 부품정보없이, 옵션정보만 입력해도 예측율 동일 수준 임\n",
    "\n",
    "# # category_var = np.load('Head_230921_2_y.npy', allow_pickle=True)\n",
    "# # print(category_var[:10])\n",
    "\n",
    "\n",
    "# X = X.astype(np.int64)\n",
    "# # category_var = category_var.astype(np.int32)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "# X_train_scaled = X_train\n",
    "# X_valid_scaled = X_test\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################\n",
    "#### 학습 / 검증용  Shuffle ############\n",
    "X = np.load('easydata_230921_2_x.npy', allow_pickle=True)\n",
    "# X = X[ : , :14]  # 테스트용 동일 모델에서는 옵션값만 있으면 학습이되는지 확인하기 위함 : 결과) 동일 모델로 구성된 데이터셋에서 부품정보없이, 옵션정보만 입력해도 예측율 동일 수준 임\n",
    "\n",
    "# category_var = np.load('Head_230921_2_y.npy', allow_pickle=True)\n",
    "# # print(category_var[:10])\n",
    "\n",
    "\n",
    "X = X.astype(np.int32)\n",
    "# category_var = category_var.astype(np.int)\n",
    "\n",
    "# num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "y = np.load('easydata_230921_2_y.npy', allow_pickle=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = y.reshape(-1)\n",
    "y = le.fit_transform(y)\n",
    "y= y. reshape(1001, 664)\n",
    "\n",
    "y = y.astype('int')\n",
    "print(X.shape , y.shape)\n",
    "X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_test)\n",
    "X_train_scaled = X_train\n",
    "X_valid_scaled = X_test\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######## Test용 ############\n",
    "# X = np.load('Head_230921_2_test_x.npy', allow_pickle=True)\n",
    "# # category_var = np.load('Head_230921_2_test_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = X.astype(np.int32)\n",
    "# # category_var = category_var.astype(np.int)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_test_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.99, random_state=42)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# # scaler = MinMaxScaler()\n",
    "# scaler = load(open('MinMax_scaler_Head_230921.pkl', 'rb'))    #Scaler 불러오기\n",
    "\n",
    "# ## X_train_scaled = scaler.fit_transform(X_train)    # Test 시, 사용하면 안됨\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "# print(X_valid_scaled.shape)\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######## NoTrain 용 ############\n",
    "# X = np.load('Head_230921_2_NoTrain_x.npy', allow_pickle=True)\n",
    "# # category_var = np.load('Head_230921_2_NoTrain_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = X.astype(np.int32)\n",
    "# # category_var = category_var.astype(np.int)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_NoTrain_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_test = X\n",
    "# y_valid = y\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# # scaler = MinMaxScaler()\n",
    "# scaler = load(open('MinMax_scaler_Head_230921.pkl', 'rb'))    #Scaler 불러오기\n",
    "\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "# # print(X_valid_scaled.shape)\n",
    "###########################################################################\n",
    "\n",
    "inputsize = X.shape[1]\n",
    "num_classes = len(set(y[0]))\n",
    "outsize = len(y[0])\n",
    "\n",
    "print(inputsize , num_classes , outsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "# class AssemblyDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = torch.tensor(X, dtype=torch.float32)\n",
    "#         self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.X[index], self.y[index]\n",
    "\n",
    "\n",
    "class AssemblyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = AssemblyDataset(X_train_scaled, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = AssemblyDataset(X_valid_scaled, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_outsize , num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(hidden_size, num_heads, num_layers, dropout)\n",
    "        # self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes*num_outsize)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        out = self.transformer(embedded, embedded)  # src와 tgt에 동일한 값을 전달합니다.\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # out = self.fc(out)\n",
    "        out = out.view(-1, num_outsize , num_classes)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 하이퍼파라미터 설정\n",
    "# input_size = inputsize  #100  # 입력 데이터의 크기\n",
    "# hidden_size = 256  # 256 임베딩 및 트랜스포머 hidden state의 크기 , 임베딩 사이즈는 num_heads로 나누어 질 수 있어야 함\n",
    "# num_layers = 4  # 트랜스포머 레이어의 수\n",
    "# num_classes = num_classes #  # 클래스의 수\n",
    "# num_outsize = outsize   \n",
    "# num_heads = 8  # 멀티 헤드 어텐션의 헤드 수\n",
    "# dropout = 1  # 드롭아웃 비율 , 소수점 입력 시 에러 발생 TypeError: 'float' object cannot be interpreted as an integer\n",
    "# batch_size = 32  # 배치 크기\n",
    "# # num_epochs = 10  # 에포크 수\n",
    "\n",
    "# # 모델 인스턴스 생성\n",
    "# model = TransformerModel(input_size, hidden_size, num_layers, num_classes, num_outsize , num_heads, dropout).to(device)\n",
    "\n",
    "# # 손실 함수와 최적화 알고리즘 설정\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = inputsize  #100  # 입력 데이터의 크기\n",
    "hidden_size = 1  # 256 임베딩 및 트랜스포머 hidden state의 크기 , 임베딩 사이즈는 num_heads로 나누어 질 수 있어야 함\n",
    "num_layers = 1  # 트랜스포머 레이어의 수\n",
    "num_classes = num_classes #  # 클래스의 수\n",
    "num_outsize = outsize   \n",
    "num_heads = 1  # 멀티 헤드 어텐션의 헤드 수\n",
    "dropout = 1  # 드롭아웃 비율 , 소수점 입력 시 에러 발생 TypeError: 'float' object cannot be interpreted as an integer\n",
    "batch_size = 128  # 배치 크기\n",
    "# num_epochs = 10  # 에포크 수\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_classes, num_outsize , num_heads, dropout).to(device)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "num_epochs = 100000\n",
    "acc_compare = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct_t = 0\n",
    "    total_t = 0\n",
    "\n",
    "    for batch_input, batch_output_t in train_loader:\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output_t = batch_output_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch_input.shape , batch_input.dtype)\n",
    "        # outputs = model(batch_input.unsqueeze(1))\n",
    "        outputs = model(batch_input)\n",
    "\n",
    "        # 손실 계산\n",
    "        # print(outputs.view(-1 , 10).shape, batch_output_t.view(-1).shape , batch_output_t.shape)\n",
    "        loss = criterion(outputs.view(-1 , num_classes), batch_output_t.view(-1) )  # 모양을 맞춤\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        predicted_t = outputs.argmax(dim=2)\n",
    "#         print(predicted , batch_output)\n",
    "        correct_t += (predicted_t == batch_output_t).sum().item()\n",
    "        total_t += batch_output_t.shape[0]*batch_output_t.shape[1]\n",
    "        # print((predicted_t == batch_output_t).sum().item() , batch_output_t.shape[0]*batch_output_t.shape[1])\n",
    "#         print(predicted_t , batch_output_t)\n",
    "\n",
    "        # print(outputs.view(-1 , num_classes).shape , predicted_t.shape , batch_output_t.shape)\n",
    "        # batch_output_t = batch_output_t.reshape(-1)\n",
    "        # predicted_t = predicted_t.reshape(-1)\n",
    "        # for i in range(len(batch_output_t)):\n",
    "        #         if batch_output_t[i] != 0:\n",
    "        #                 total_t+=1\n",
    "        #                 if batch_output_t[i] == predicted_t[i]:\n",
    "        #                         correct_t+=1\n",
    "\n",
    "#     average_loss = total_loss / len(train_loader)\n",
    "#     acc_train = correct_t / total_t\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f} , acc : { acc_train :.4f} , { correct_t , total_t}')\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_t / (total_t)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f} , acc : { accuracy :.4f} , { correct_t , total_t}')\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in valid_loader:\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_output = batch_output.to(device)\n",
    "            outputs = model(batch_input)\n",
    "            predicted = outputs.argmax(dim=2)\n",
    "    #         print(predicted , batch_output)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "    #         print(batch_output.size(0))\n",
    "            total += batch_output.shape[0]*batch_output.shape[1]\n",
    "\n",
    "\n",
    "        #     batch_output = batch_output.reshape(-1)\n",
    "        #     predicted = predicted.reshape(-1)\n",
    "        #     for i in range(len(batch_output)):\n",
    "        #         if batch_output[i] != 0:\n",
    "        #                 total_v+=1\n",
    "        #                 if batch_output[i] == predicted[i]:\n",
    "        #                         correct_v+=1\n",
    "\n",
    "        # acc_val = correct_v / total_v\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}% , {correct , total}')\n",
    "    if accuracy > acc_compare:\n",
    "        acc_compare = accuracy\n",
    "        torch.save(model.state_dict(), \"Transformer_230926_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m \u001b[39m# 순전파\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m outputs \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     13\u001b[0m \u001b[39m# print(outputs.shape , labels.shape)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# loss = criterion(outputs, labels)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m , num_classes), labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) )  \u001b[39m# 모양을 맞춤\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1531\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1526\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1529\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1530\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1531\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1533\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1534\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src):\n\u001b[1;32m---> 11\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(src)\n\u001b[0;32m     12\u001b[0m     embedded \u001b[39m=\u001b[39m embedded\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     14\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(embedded, embedded)  \u001b[39m# src와 tgt에 동일한 값을 전달합니다.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1531\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1526\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1529\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1530\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1531\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1533\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1534\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2238\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2232\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2237\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2238\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파\n",
    "        outputs = model(data)\n",
    "        # print(outputs.shape , labels.shape)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = criterion(outputs.view(-1 , num_classes), labels.view(-1) )  # 모양을 맞춤\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 현재 에포크의 평균 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_data = torch.randint(input_size, (100, 20))  # (데이터 수, 시퀀스 길이)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)\n",
    "    _, predicted_classes = torch.max(predictions, 1)\n",
    "\n",
    "    # 예측 결과 출력\n",
    "    print(f'Predicted classes: {predicted_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:217: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  encoder_layer.self_attn.batch_first was not True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[86.7512, 31.9411, 27.0198,  ...,  9.8401, 91.7603, 35.5223],\n",
      "        [35.9111, 85.2471, 37.3877,  ...,  2.5263, 68.7031, 66.2507],\n",
      "        [46.4888, 91.8918, 56.6178,  ..., 99.1684, 97.6022, 68.4687],\n",
      "        [88.8976, 94.6632, 20.2882,  ..., 68.7171, 87.6440,  3.2913],\n",
      "        [49.7951, 57.5136,  1.7006,  ...,  8.3887, 30.8383, 56.4679]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 트랜스포머 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(hidden_size, num_heads, num_layers, dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.embedding(src.long())  # 입력 텐서를 Long 타입으로 변환\n",
    "        embedded_tgt = self.embedding(tgt.long())  # 입력 텐서를 Long 타입으로 변환\n",
    "\n",
    "        embedded_src = embedded_src.permute(1, 0, 2)  # 차원 순서 변경\n",
    "        embedded_tgt = embedded_tgt.permute(1, 0, 2)  # 차원 순서 변경\n",
    "\n",
    "        out = self.transformer(embedded_src, embedded_tgt)\n",
    "        out = out.permute(1, 0, 2)  # 차원 순서 변경\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = 100  # 입력 데이터의 크기\n",
    "hidden_size = 256  # 임베딩 및 트랜스포머 hidden state의 크기\n",
    "num_layers = 4  # 트랜스포머 레이어의 수\n",
    "num_classes = 3  # 클래스의 수\n",
    "num_heads = 8  # 멀티 헤드 어텐션의 헤드 수\n",
    "dropout = 1  # 드롭아웃 비율\n",
    "batch_size = 32  # 배치 크기\n",
    "num_epochs = 10  # 에포크 수\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_classes, num_heads, dropout)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 데이터셋과 레이블 생성\n",
    "train_data = torch.rand(100, 3320).mul(input_size).float()  # (데이터 수, 시퀀스 길이)\n",
    "train_labels = torch.randint(num_classes, (100,))\n",
    "print(train_data[:5])\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파\n",
    "        outputs = model(data, data)  # src와 tgt에 동일한 데이터를 사용\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 현재 에포크의 평균 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_data = torch.rand(100, 20).mul(input_size).long()  # (데이터 수, 시퀀스 길이)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data, test_data)  # src와 tgt에 동일한 데이터를 사용\n",
    "    _, predicted_classes = torch.max(predictions, 1)\n",
    "\n",
    "    # 예측 결과 출력\n",
    "    print(f'Predicted classes: {predicted_classes}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:217: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  encoder_layer.self_attn.batch_first was not True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.3349\n",
      "Epoch [2/10], Loss: 1.1136\n",
      "Epoch [3/10], Loss: 1.1206\n",
      "Epoch [4/10], Loss: 1.1243\n",
      "Epoch [5/10], Loss: 1.1031\n",
      "Epoch [6/10], Loss: 1.1084\n",
      "Epoch [7/10], Loss: 1.1051\n",
      "Epoch [8/10], Loss: 1.1089\n",
      "Epoch [9/10], Loss: 1.1164\n",
      "Epoch [10/10], Loss: 1.1207\n",
      "Predicted classes: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 트랜스포머 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(hidden_size, num_heads), num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(hidden_size, num_heads), num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.embedding(src.long())  # 입력 텐서를 Long 타입으로 변환\n",
    "        embedded_tgt = self.embedding(tgt.long())  # 입력 텐서를 Long 타입으로 변환\n",
    "\n",
    "        embedded_src = embedded_src.permute(1, 0, 2)  # 차원 순서 변경\n",
    "        embedded_tgt = embedded_tgt.permute(1, 0, 2)  # 차원 순서 변경\n",
    "\n",
    "        encoder_out = self.encoder(embedded_src)\n",
    "        decoder_out = self.decoder(embedded_tgt, encoder_out)\n",
    "\n",
    "        decoder_out = decoder_out.permute(1, 0, 2)  # 차원 순서 변경\n",
    "\n",
    "        out = self.fc(decoder_out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = 100  # 입력 데이터의 크기\n",
    "hidden_size = 256  # 임베딩 및 트랜스포머 hidden state의 크기\n",
    "num_layers = 4  # 트랜스포머 레이어의 수\n",
    "num_classes = 3  # 클래스의 수\n",
    "num_heads = 8  # 멀티 헤드 어텐션의 헤드 수\n",
    "dropout = 1  # 드롭아웃 비율\n",
    "batch_size = 32  # 배치 크기\n",
    "num_epochs = 10  # 에포크 수\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_classes, num_heads, dropout)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 데이터셋과 레이블 생성\n",
    "train_data = torch.rand(1000, 20).mul(input_size).long()  # (데이터 수, 시퀀스 길이)\n",
    "train_labels = torch.randint(num_classes, (1000,))\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파\n",
    "        outputs = model(data, data)  # src와 tgt에 동일한 데이터를 사용\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 현재 에포크의 평균 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_data = torch.rand(100, 20).mul(input_size).long()  # (데이터 수, 시퀀스 길이)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data, test_data)  # src와 tgt에 동일한 데이터를 사용\n",
    "    _, predicted_classes = torch.max(predictions, 1)\n",
    "\n",
    "    # 예측 결과 출력\n",
    "    print(f'Predicted classes: {predicted_classes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     74\u001b[0m \u001b[39m# 역전파 및 가중치 업데이트\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     76\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     78\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    483\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    484\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    490\u001b[0m     )\n\u001b[1;32m--> 491\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    492\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    493\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    201\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    205\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    206\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 트랜스포머 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(hidden_size, num_heads, num_layers, dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        out = self.transformer(embedded, embedded)  # src와 tgt에 동일한 값을 전달합니다.\n",
    "        out = out.permute(1, 0, 2)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = 100  # 입력 데이터의 크기\n",
    "hidden_size = 256  # 임베딩 및 트랜스포머 hidden state의 크기\n",
    "num_layers = 4  # 트랜스포머 레이어의 수\n",
    "num_classes = 3  # 클래스의 수\n",
    "num_heads = 8  # 멀티 헤드 어텐션의 헤드 수\n",
    "dropout = 1  # 드롭아웃 비율 , 소수점 입력 시 에러 발생 TypeError: 'float' object cannot be interpreted as an integer\n",
    "batch_size = 32  # 배치 크기\n",
    "num_epochs = 10  # 에포크 수\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = TransformerModel(input_size, hidden_size, num_layers, num_classes, num_heads, dropout)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 데이터셋과 레이블 생성\n",
    "train_data = torch.randint(input_size, (1000, 20))  # (데이터 수, 시퀀스 길이)\n",
    "train_labels = torch.randint(num_classes, (1000,))\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 현재 에포크의 평균 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_data = torch.randint(input_size, (100, 20))  # (데이터 수, 시퀀스 길이)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)\n",
    "    _, predicted_classes = torch.max(predictions, 1)\n",
    "\n",
    "    # 예측 결과 출력\n",
    "    print(f'Predicted classes: {predicted_classes}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
