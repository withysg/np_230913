{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1632, 9014) (1632, 1000)\n",
      "9014 9 1000\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset _Mix\n",
    "# X = np.load('EBR32094107_BOT_Head100_x.npy', allow_pickle=True)  \n",
    "## X = X[:, :15] 으로 학습 시, Epoch [31/100000], Train Loss: 0.00883, Train Acc: 0.911, Valid Loss: 0.02390, Valid Acc: 0.902\n",
    "# y = np.load('All_Head_Simple_NoMix_y.npy')\n",
    "# category_var = np.load('EBR32094107_BOT_Head100_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = np.load('All_Head_Simple_NoMix_x.npy')  # 여기 데이터셋으로 검증하면 정확도 0.89 임\n",
    "# category_var = np.load('All_Head_Simple_NoMix_y.npy')\n",
    "\n",
    "# X = np.load('All_Head_Simple_x.npy')\n",
    "# category_var = np.load('All_Head_Simple_y.npy')\n",
    "\n",
    "# X = np.load('D:/py/DGS/EBR32094107_BOT_Head66_x.npy', allow_pickle=True)\n",
    "# category_var = np.load('D:/py/DGS/EBR32094107_BOT_Head66_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = np.load('D:/py/DGS/EBR32094107_BOT_Head_NoMix_x.npy', allow_pickle=True)\n",
    "# category_var = np.load('D:/py/DGS/EBR32094107_BOT_Head_NoMix_y.npy', allow_pickle=True)\n",
    "\n",
    "######################################################\n",
    "#### 학습 / 검증용 #########\n",
    "X = np.load('Head_230927_1_x.npy', allow_pickle=True)\n",
    "# X = X[ : , :14]  # 테스트용 동일 모델에서는 옵션값만 있으면 학습이되는지 확인하기 위함 : 결과) 동일 모델로 구성된 데이터셋에서 부품정보없이, 옵션정보만 입력해도 예측율 동일 수준 임\n",
    "\n",
    "# category_var = np.load('Head_230921_2_y.npy', allow_pickle=True)\n",
    "# print(category_var[:10])\n",
    "\n",
    "\n",
    "X = X.astype(np.float)\n",
    "# category_var = category_var.astype(np.int32)\n",
    "\n",
    "# num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "y = np.load('Head_230927_1_y.npy', allow_pickle=True)\n",
    "y = y.astype(np.int32)\n",
    "print(X.shape , y.shape)\n",
    "X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "le = LabelEncoder()\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_test)\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######## Test용 ############\n",
    "# X = np.load('Head_230921_2_test_x.npy', allow_pickle=True)\n",
    "# # category_var = np.load('Head_230921_2_test_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = X.astype(np.int32)\n",
    "# # category_var = category_var.astype(np.int)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_test_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.99, random_state=42)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# # scaler = MinMaxScaler()\n",
    "# scaler = load(open('MinMax_scaler_Head_230921.pkl', 'rb'))    #Scaler 불러오기\n",
    "\n",
    "# ## X_train_scaled = scaler.fit_transform(X_train)    # Test 시, 사용하면 안됨\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "# print(X_valid_scaled.shape)\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######## NoTrain 용 ############\n",
    "# X = np.load('Head_230921_2_NoTrain_x.npy', allow_pickle=True)\n",
    "# # category_var = np.load('Head_230921_2_NoTrain_y.npy', allow_pickle=True)\n",
    "\n",
    "# X = X.astype(np.int32)\n",
    "# # category_var = category_var.astype(np.int)\n",
    "\n",
    "# # num_category = len(np.unique(category_var))+1 ## 유니크 범주 개수\n",
    "# # identity_mat = np.eye(num_category) ## 단위 행렬\n",
    "# # y = identity_mat[category_var] ## 범주에 대응하는 행 추출\n",
    "\n",
    "# y = np.load('Head_230921_2_NoTrain_y.npy', allow_pickle=True)\n",
    "# y = y.astype(np.int32)\n",
    "# print(X.shape , y.shape)\n",
    "# X_test = X\n",
    "# y_valid = y\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# # scaler = StandardScaler()\n",
    "# # scaler = MinMaxScaler()\n",
    "# scaler = load(open('MinMax_scaler_Head_230921.pkl', 'rb'))    #Scaler 불러오기\n",
    "\n",
    "# X_valid_scaled = scaler.transform(X_test)\n",
    "# # print(X_valid_scaled.shape)\n",
    "###########################################################################\n",
    "\n",
    "inputsize = X.shape[1]\n",
    "num_classes = len(set(y[0]))\n",
    "outsize = len(y[0])\n",
    "\n",
    "print(inputsize , num_classes , outsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class AssemblyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = AssemblyDataset(X_train_scaled, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = AssemblyDataset(X_valid_scaled, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers , outsize, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers , batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, outsize*num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "#         out = out[:, -1, :]  # 마지막 시퀀스 스텝의 출력만 사용\n",
    "        out = self.fc(out)\n",
    "        out = out.view(-1, outsize , num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "input_size = inputsize\n",
    "hidden_size = 1000\n",
    "num_layers = 2\n",
    "num_classes = num_classes\n",
    "model = LSTMModel(input_size, hidden_size, num_layers , outsize , num_classes).to(device)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100000], Loss: 0.9736 , acc : 63.5296 , (829061, 1305000)\n",
      "Validation Accuracy: 69.19% , (226248, 327000)\n",
      "Epoch [2/100000], Loss: 0.6971 , acc : 71.1025 , (927888, 1305000)\n",
      "Validation Accuracy: 70.44% , (230336, 327000)\n",
      "Epoch [3/100000], Loss: 0.6692 , acc : 71.4851 , (932881, 1305000)\n",
      "Validation Accuracy: 70.99% , (232152, 327000)\n",
      "Epoch [4/100000], Loss: 0.6636 , acc : 71.6261 , (934721, 1305000)\n",
      "Validation Accuracy: 70.75% , (231345, 327000)\n",
      "Epoch [5/100000], Loss: 0.6610 , acc : 71.6843 , (935480, 1305000)\n",
      "Validation Accuracy: 71.37% , (233373, 327000)\n",
      "Epoch [6/100000], Loss: 0.6593 , acc : 71.6808 , (935435, 1305000)\n",
      "Validation Accuracy: 71.01% , (232191, 327000)\n",
      "Epoch [7/100000], Loss: 0.6576 , acc : 71.7897 , (936855, 1305000)\n",
      "Validation Accuracy: 71.36% , (233358, 327000)\n",
      "Epoch [8/100000], Loss: 0.6571 , acc : 71.7785 , (936709, 1305000)\n",
      "Validation Accuracy: 71.56% , (233996, 327000)\n",
      "Epoch [9/100000], Loss: 0.6566 , acc : 71.8116 , (937142, 1305000)\n",
      "Validation Accuracy: 71.57% , (234027, 327000)\n",
      "Epoch [10/100000], Loss: 0.6539 , acc : 71.9135 , (938471, 1305000)\n",
      "Validation Accuracy: 70.90% , (231851, 327000)\n",
      "Epoch [11/100000], Loss: 0.6548 , acc : 71.7590 , (936455, 1305000)\n",
      "Validation Accuracy: 71.25% , (232982, 327000)\n",
      "Epoch [12/100000], Loss: 0.6537 , acc : 71.8364 , (937465, 1305000)\n",
      "Validation Accuracy: 70.73% , (231292, 327000)\n",
      "Epoch [13/100000], Loss: 0.6519 , acc : 71.9503 , (938951, 1305000)\n",
      "Validation Accuracy: 71.69% , (234436, 327000)\n",
      "Epoch [14/100000], Loss: 0.6529 , acc : 71.9247 , (938617, 1305000)\n",
      "Validation Accuracy: 71.58% , (234073, 327000)\n",
      "Epoch [15/100000], Loss: 0.6521 , acc : 71.9418 , (938841, 1305000)\n",
      "Validation Accuracy: 71.38% , (233397, 327000)\n",
      "Epoch [16/100000], Loss: 0.6504 , acc : 72.1231 , (941207, 1305000)\n",
      "Validation Accuracy: 71.04% , (232314, 327000)\n",
      "Epoch [17/100000], Loss: 0.6513 , acc : 72.0036 , (939647, 1305000)\n",
      "Validation Accuracy: 71.88% , (235050, 327000)\n",
      "Epoch [18/100000], Loss: 0.6507 , acc : 72.1043 , (940961, 1305000)\n",
      "Validation Accuracy: 71.74% , (234575, 327000)\n",
      "Epoch [19/100000], Loss: 0.6505 , acc : 71.9807 , (939348, 1305000)\n",
      "Validation Accuracy: 71.35% , (233320, 327000)\n",
      "Epoch [20/100000], Loss: 0.6505 , acc : 72.0637 , (940431, 1305000)\n",
      "Validation Accuracy: 71.15% , (232668, 327000)\n",
      "Epoch [21/100000], Loss: 0.6501 , acc : 71.9657 , (939153, 1305000)\n",
      "Validation Accuracy: 71.70% , (234467, 327000)\n",
      "Epoch [22/100000], Loss: 0.6493 , acc : 72.0389 , (940108, 1305000)\n",
      "Validation Accuracy: 71.53% , (233887, 327000)\n",
      "Epoch [23/100000], Loss: 0.6500 , acc : 72.0474 , (940219, 1305000)\n",
      "Validation Accuracy: 70.83% , (231629, 327000)\n",
      "Epoch [24/100000], Loss: 0.6494 , acc : 71.9369 , (938776, 1305000)\n",
      "Validation Accuracy: 71.09% , (232475, 327000)\n",
      "Epoch [25/100000], Loss: 0.6496 , acc : 72.0474 , (940218, 1305000)\n",
      "Validation Accuracy: 71.76% , (234649, 327000)\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "num_epochs = 100000\n",
    "acc_compare = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct_t = 0\n",
    "    total_t = 0\n",
    "\n",
    "    for batch_input, batch_output_t in train_loader:\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output_t = batch_output_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순방향 전파\n",
    "        outputs = model(batch_input.unsqueeze(1))  # LSTM 입력 형태로 변환\n",
    "\n",
    "        # 손실 계산\n",
    "        # print(outputs.view(-1 , 10).shape, batch_output_t.view(-1).shape , batch_output_t.shape)\n",
    "        loss = criterion(outputs.view(-1 , num_classes), batch_output_t.view(-1) )  # 모양을 맞춤\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        predicted_t = outputs.argmax(dim=2)\n",
    "#         print(predicted , batch_output)\n",
    "        correct_t += (predicted_t == batch_output_t).sum().item()\n",
    "        total_t += batch_output_t.shape[0]*batch_output_t.shape[1]\n",
    "        # print((predicted_t == batch_output_t).sum().item() , batch_output_t.shape[0]*batch_output_t.shape[1])\n",
    "#         print(predicted_t , batch_output_t)\n",
    "\n",
    "        # print(outputs.view(-1 , num_classes).shape , predicted_t.shape , batch_output_t.shape)\n",
    "        # batch_output_t = batch_output_t.reshape(-1)\n",
    "        # predicted_t = predicted_t.reshape(-1)\n",
    "        # for i in range(len(batch_output_t)):\n",
    "        #         if batch_output_t[i] != 0:\n",
    "        #                 total_t+=1\n",
    "        #                 if batch_output_t[i] == predicted_t[i]:\n",
    "        #                         correct_t+=1\n",
    "\n",
    "#     average_loss = total_loss / len(train_loader)\n",
    "#     acc_train = correct_t / total_t\n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f} , acc : { acc_train :.4f} , { correct_t , total_t}')\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_t / (total_t)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f} , acc : { accuracy :.4f} , { correct_t , total_t}')\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in valid_loader:\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_output = batch_output.to(device)\n",
    "            outputs = model(batch_input.unsqueeze(1))\n",
    "            predicted = outputs.argmax(dim=2)\n",
    "    #         print(predicted , batch_output)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "    #         print(batch_output.size(0))\n",
    "            total += batch_output.shape[0]*batch_output.shape[1]\n",
    "\n",
    "\n",
    "        #     batch_output = batch_output.reshape(-1)\n",
    "        #     predicted = predicted.reshape(-1)\n",
    "        #     for i in range(len(batch_output)):\n",
    "        #         if batch_output[i] != 0:\n",
    "        #                 total_v+=1\n",
    "        #                 if batch_output[i] == predicted[i]:\n",
    "        #                         correct_v+=1\n",
    "\n",
    "        # acc_val = correct_v / total_v\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}% , {correct , total}')\n",
    "    if accuracy > acc_compare:\n",
    "        acc_compare = accuracy\n",
    "        torch.save(model.state_dict(), \"LSTM_Head_All_230927_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_t = batch_output.reshape(-1)\n",
    "pre_t = predicted.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :  1.0\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "c = 0\n",
    "t_list = []\n",
    "c_list = []\n",
    "for i in range(len(cor_t)):\n",
    "    if cor_t[i] != 0:\n",
    "        t+=1\n",
    "        Y_val_np = cor_t[i].cpu().numpy()\n",
    "        t_list.append(Y_val_np)\n",
    "        pred_np = pre_t[i].cpu().numpy()\n",
    "        c_list.append(pred_np)\n",
    "        if cor_t[i] == pre_t[i]:\n",
    "            c+=1\n",
    "\n",
    "print('acc : ' , c/t)\n",
    "\n",
    "# pred_np = pred.cpu().numpy()\n",
    "# pred_np = pred_np.reshape(-1)\n",
    "# Y_val_np = Y_val.cpu().numpy()\n",
    "# Y_val_np = Y_val_np.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(t_list)):\n",
    "    print(t_list[i] ,c_list[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_list.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
